# EDLM_Whisper_Boost
Выпускной проект курса "Эффективные модели глубокого обучения" по теме ускорения модели Whisper

Результаты метрик можно увидеть в файле [whisper_metric.json](metrics%2Fwhisper_metric.json)

## Профилировка и бейзлайн
[whisper_baseline.ipynb](notebooks%2Fwhisper_baseline.ipynb)

Бейзлайн метрик модели Whisper Large:

| Device | WER   | CER   | Avg Time per Audio (s) |
|--------|-------|-------|------------------------|
| CPU    | 0.4403 | 0.1583 | 8.7137                 |
| GPU    | 0.4403 | 0.1583 | 0.8386                 |

Профилировка показывает следующее:
1. Подавляющее большинство операций на CPU и значимая часть на GPU - операции aten::addmm типа output = beta * input + alpha * (mat1 @ mat2), которые по сути являются умножениями матриц в линейном слое. В теории, прунинг должен уменьшать количество таких операций, а, значит и положительно влиять на скорость инференса.
2. Самые тяжелые операции на GPU aten::_efficient_attention_forward и fmha_cutlassF_f32_aligned_64x64_rf_sm80 - эффективное вычисление аттеншена и на них потенциально может помочь квантизация или mixed precision.

## Pruning
[torch_pruning.ipynb](notebooks%2Ftorch_pruning.ipynb)

В качестве прунинга были использованы:
1. Магнитудный прунинг. Отработал на CPU и GPU, значительного ускорения добились только на видеокарте (0.8 -> 0.6 секунд на запись)
2. L2 прунинг. Разрушил модель при проходе, решено не использовать.
3. Варианты прунинга с удалением весов реализовать не удалось.

Для итогового варианта будут использоваться следующие виды прунинга:
1. Магнитудный для GPU без квантизации, т.к. статическая квантизация на GPU была получена только из готовой реализации (см. следующий пункт).
2. Магнитудный прунинг для CPU с квантизацией и компиляцией модели (см. последний пункт). Ожидается приближение к real-time по метрике.

Итоговые метрики:

| Конфигурация                                      | total_samples | avg_wer  | avg_cer  | avg_time_per_audio | total_time  |
|---------------------------------------------------|---------------|----------|----------|--------------------|-------------|
| large-v3_cuda_global_magnitude_pruning_0.9       | 793           | 0.4415   | 0.1584   | 0.6126             | 485.79      |
| large-v3_cuda_global_magnitude_pruning_0.85      | 793           | 0.4437   | 0.1627   | 0.6092             | 483.11      |
| large-v3_cuda_global_magnitude_pruning_0.81      | 793           | 0.4410   | 0.1619   | 0.6089             | 482.90      |
| large-v3_cpu_global_magnitude_pruning_0.9        | 793           | 0.4415   | 0.1584   | 7.4018             | 5869.63     |
| large-v3_cpu_global_magnitude_pruning_0.85       | 793           | 0.4437   | 0.1627   | 7.3621             | 5838.17     |
| large-v3_cpu_global_magnitude_pruning_0.81       | 793           | 0.4410   | 0.1619   | 7.4373             | 5897.79     |


## Quantization baseline
[quantization_baseline.ipynb](notebooks%2Fquantization_baseline.ipynb)

Были проведены эксперименты следующего вида:

0. Бесплатное ускорение - torch.compile.
1. Статическая квантизация. Не завелась, т.к. Whisper при generate вызывает torch.nn.F слои, вокруг которых не работает обёртка torch.ao. Monkey-patch не помог, отложено.
2. Динамическая квантизация линейных слоёв на CPU.
3. Статическая квантизация bitsandbytes на GPU.
4. torch.autocast как дешёвая альтернатива.

### torch.compile
Напрямую не относится к квантизации, но было показано, что даёт статистически значимое ускорение на CPU и GPU. Используется в связке с квантизацией для лучшего результата инференса. На пачке из 1 аудиозаписи получено значимое ускорение.

### Статическая квантизация
Проведено тестирование PTQ Conv1d, LayerNorm и Linear слоёв на fbgemm бэкэнде. Квантованная модель по общему весу была равна 30% исходной. Инференс не запустился по указанным выше причинам.

### Динамическая квантизация
Проведено тестирование PTQ Linear слоёв в int8 на базовом конфиге на CPU. При незначительном снижении качества (порядка 8% WER) инференс ускорен на 2.1 сек.

### Статическая квантизация bitsandbytes
Произведены замеры предварительно квантованного Whisper на GPU. Инференс замедлился примерно в 3.5 раза. В профилировщике видны новые накладные расходы на 8bit matmul, что и является основной причиной - 3х4096 вызовов, относящихся к этой операции создают внушительный накладняк по CPU (на сами вызовы операций и CUDA kernel), что и замедляет работу инференса. 

### torch.autocast
Абсолютно не применим на CPU, но, в случае с GPU показывает незначительное отставание на сами касты. Потенциально выгоден на большом батче аудиозаписей, где квантованная модель на GPU покажет то же самое отставание по CPU.

### Итоговые метрики
| Model Variant                     | WER    | CER    | Avg Time per Audio| Total Time |
|----------------------------------|--------|--------|-------------------|------------|
| large-v3                         | 0.4403 | 0.1583 | 7.9487            | 6303.28    |
| large-v3_cuda                    | 0.4403 | 0.1583 | 0.8386            | 665.03     |
| large-v3_cpu_quanted             | 0.4740 | 0.1664 | 5.8057            | 4603.92    |
| large-v3_cuda_quanted            | 0.4472 | 0.1586 | 3.0069            | 2384.43    |
| large-v3_cuda_autocast_compile   | 0.4426 | 0.1589 | 0.8967            | 711.10     |

## Quantization advanced configs
[whisper_quantization.ipynb](notebooks%2Fwhisper_quantization.ipynb)

Дополнительно были проверены конфигурации из документации торча для квантизации. Проверялись интовые конфиги разной вычислительной точности, велись попытки выполнить квантизацию во флотовых конфигах (неудачные).
Результаты можно увидеть здесь:

| Конфигурация                             | total_samples | avg_wer  | avg_cer  | avg_time_per_audio | total_time  |
|------------------------------------------|---------------|----------|----------|--------------------|-------------|
| without_quantization                     | 793           | 0.4403   | 0.1583   | 5.4567             | 4327.14     |
| Int8DynamicActivationInt8WeightConfig    | 793           | 0.4480   | 0.1582   | 4.8465             | 3843.29     |
| Int8DynamicActivationInt4WeightConfig    | 793           | 0.4498   | 0.1610   | 10.5658            | 8378.67     |

## Final call
[final_call.ipynb](notebooks%2Ffinal_call.ipynb)

За финальный эксперимент была выбрана модель с магнитудным прунингом 0.81, линейные слои которой проквантованы в int8 и проведена последующая компиляция.

Модель ожидаемо показала лучший результат - 5.08с средняя скорость обработки при средней длительности записи в датасете - 4.09 ~80% от realtime.

Итоговые результаты проекта:

| Конфигурация                                      | total_samples | avg_wer  | avg_cer  | avg_time_per_audio | total_time |
|---------------------------------------------------|---------------|----------|----------|--------------------|------------|
| large-v3                                          | 793           | 0.4403   | 0.1583   | 7.95               | 6303.28    |
| large-v3_cuda                                     | 793           | 0.4403   | 0.1583   | 0.84               | 665.03     |
| large-v3_cpu_quanted                              | 793           | 0.4740   | 0.1664   | 5.81               | 4603.92    |
| large-v3_cpu_global_magnitude_pruning_0.81       | 793           | 0.4410   | 0.1619   | 7.44               | 5897.79    |
| large-v3_cpu_quanted_pruned                       | 793           | 0.4815   | 0.1664   | 5.08               | 4031.35    |
