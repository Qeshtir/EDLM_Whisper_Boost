# EDLM_Whisper_Boost
Выпускной проект курса "Эффективные модели глубокого обучения" по теме ускорения модели Whisper

## Профилировка и бейзлайн
Бейзлайн метрик модели Whisper Large:

| Device | WER   | CER   | Avg Time per Audio (s) |
|--------|-------|-------|------------------------|
| CPU    | 0.4403 | 0.1583 | 8.7137                 |
| GPU    | 0.4403 | 0.1583 | 0.8386                 |

Профилировка показывает следующее:
1. Подавляющее большинство операций на CPU и значимая часть на GPU - операции aten::addmm типа output = beta * input + alpha * (mat1 @ mat2), которые по сути являются умножениями матриц в линейном слое. В теории, прунинг должен уменьшать количество таких операций, а, значит и положительно влиять на скорость инференса.
2. Самые тяжелые операции на GPU aten::_efficient_attention_forward и fmha_cutlassF_f32_aligned_64x64_rf_sm80 - эффективное вычисление аттеншена и на них потенциально может помочь квантизация или mixed precision.

## Pruning


## Quantization baseline
Были проведены эксперименты следующего вида:

0. Бесплатное ускорение - torch.compile.
1. Статическая квантизация. Не завелась, т.к. Whisper при generate вызывает torch.nn.F слои, вокруг которых не работает обёртка torch.ao. Monkey-patch не помог, отложено.
2. Динамическая квантизация линейных слоёв на CPU.
3. Статическая квантизация bitsandbytes на GPU.
4. torch.autocast как дешёвая альтернатива.

### torch.compile
Напрямую не относится к квантизации, но было показано, что даёт статистически значимое ускорение на CPU и GPU. Используется в связке с квантизацией для лучшего результата инференса. На пачке из 1 аудиозаписи получено значимое ускорение.

### Статическая квантизация
Проведено тестирование PTQ Conv1d, LayerNorm и Linear слоёв на fbgemm бэкэнде. Квантованная модель по общему весу была равна 30% исходной. Инференс не запустился по указанным выше причинам.

### Динамическая квантизация
Проведено тестирование PTQ Linear слоёв в int8 на базовом конфиге на CPU. При незначительном снижении качества (порядка 8% WER) инференс ускорен на 2.1 сек.

### Статическая квантизация bitsandbytes
Произведены замеры предварительно квантованного Whisper на GPU. Инференс замедлился примерно в 3.5 раза. В профилировщике видны новые накладные расходы на 8bit matmul, что и является основной причиной - 3х4096 вызовов, относящихся к этой операции создают внушительный накладняк по CPU (на сами вызовы операций и CUDA kernel), что и замедляет работу инференса. 

### torch.autocast
Абсолютно не применим на CPU, но, в случае с GPU показывает незначительное отставание на сами касты. Потенциально выгоден на большом батче аудиозаписей, где квантованная модель на GPU покажет то же самое отставание по CPU.

### Итоговые метрики
| Model Variant                     | WER    | CER    | Avg Time per Audio (s) | Total Time (s) |
|----------------------------------|--------|--------|------------------------|----------------|
| large-v3                         | 0.4403 | 0.1583 | 7.9487                 | 6303.28        |
| large-v3_cuda                    | 0.4403 | 0.1583 | 0.8386                 | 665.03         |
| large-v3_cpu_quanted             | 0.4740 | 0.1664 | 5.8057                 | 4603.92        |
| large-v3_cuda_quanted            | 0.4472 | 0.1586 | 3.0069                 | 2384.43        |
| large-v3_cuda_autocast_compile   | 0.4426 | 0.1589 | 0.8967                 | 711.10         |

