{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f0b99c-4578-44b6-a456-61b7f912bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch.quantization\n",
    "import torch.ao.quantization as tq\n",
    "import torch.nn.functional as F\n",
    "import inspect\n",
    "import types\n",
    "from types import FunctionType\n",
    "from functools import lru_cache\n",
    "import accelerate\n",
    "import gc\n",
    "\n",
    "import jiwer\n",
    "from jiwer import (\n",
    "    Compose,\n",
    "    ToLowerCase,\n",
    "    RemoveMultipleSpaces,\n",
    "    Strip,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652d098-c5a7-4e57-a3f4-bb0ec2ecc698",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "В этом ноутбуке тестируется post-training квантизация как один из простейших методов, не требующих пересборки архитектуры модели. Ожидается, что квантизация даст прирост по скорости в простых операциях матричного умножения и в аттеншн блоках модели.\n",
    "\n",
    "Все значения профилировщика были сняты в прогоне на чистовую вне изолированной среды, поэтому значения могут отличаться, но статистически сводятся к итоговым метрикам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae18e1e5-24fc-409b-babd-cef272baeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\" Prints the real size of the model \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58fb1fa-874c-410a-b47d-180b6115d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asr_metrics(hypothesis: str, reference: str):\n",
    "    tr = Compose([ToLowerCase(), RemoveMultipleSpaces(), Strip()])\n",
    "\n",
    "    ref_tr = tr(reference)\n",
    "    hyp_tr = tr(hypothesis)\n",
    "\n",
    "    out = jiwer.process_words(ref_tr, hyp_tr)\n",
    "    wer = out.wer\n",
    "    # S, D, I = out.substitutions, out.deletions, out.insertions\n",
    "\n",
    "    cer = jiwer.cer(ref_tr, hyp_tr) # ?????\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059ea448-06fe-4a2b-bc43-1ced9c20b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_sample(sample_idx=0, trace_path=\"whisper_perfetto_large-v3.json\", sort_by=\"cpu_time_total\", model=None):\n",
    "    example = dataset[sample_idx]\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "        with record_function(\"whisper.generate\"):\n",
    "            predicted_ids = model.generate(inputs, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "    prof.export_chrome_trace(trace_path)\n",
    "    print(f\"Perfetto trace saved to {trace_path}\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=sort_by,\n",
    "        row_limit=10\n",
    "    ))\n",
    "    return processor.decode(predicted_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca2588d-c490-4ed8-a4d2-63cdccf5079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bond005/sberdevices_golos_10h_crowd\", split=\"validation\") #, split=\"test\")\n",
    "# dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afaa4d97-cb9c-4238-bbde-f955c671a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 6174.372281\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af428acb-7dd7-4215-8591-156f44a1f954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperForConditionalGeneration(\n",
      "  (model): WhisperModel(\n",
      "    (encoder): WhisperEncoder(\n",
      "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (embed_positions): Embedding(1500, 1280)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x WhisperEncoderLayer(\n",
      "          (self_attn): WhisperAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): WhisperDecoder(\n",
      "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
      "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x WhisperDecoderLayer(\n",
      "          (self_attn): WhisperAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): WhisperAttention(\n",
      "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на архитектуру модели\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea957dd-af6f-41e5-a42b-01a8f9634045",
   "metadata": {},
   "source": [
    "# CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c7836-8d8c-44cc-a4cf-debab8b551fb",
   "metadata": {},
   "source": [
    "## Бесплатное ускорение с torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e6e234-5413-4316-84e6-078552749162",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46cab216-b2b7-47ff-9349-a51e4218f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_compiled.json\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     whisper.generate         3.81%     326.723ms       100.00%        8.569s        8.569s         112 B      -6.85 GB             1  \n",
      "                                         aten::linear         0.59%      50.416ms        72.62%        6.222s       1.514ms       2.55 GB           0 B          4111  \n",
      "                                          aten::addmm        59.14%        5.068s        60.90%        5.219s       1.469ms       2.09 GB       2.09 GB          3552  \n",
      "                   aten::scaled_dot_product_attention         0.08%       6.716ms        13.37%        1.145s       1.154ms     240.00 MB      -3.75 MB           992  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu        12.87%        1.103s        13.29%        1.139s       1.148ms     243.75 MB    -155.99 MB           992  \n",
      "                                         aten::matmul         0.10%       8.291ms        10.38%     889.642ms       1.591ms     475.12 MB           0 B           559  \n",
      "                                             aten::mm        10.22%     875.963ms        10.23%     876.578ms       1.568ms     475.12 MB     475.12 MB           559  \n",
      "                                          aten::copy_         4.90%     419.945ms         4.90%     419.945ms      80.759us           0 B           0 B          5200  \n",
      "                                     aten::contiguous         0.02%       1.544ms         3.40%     291.352ms     648.890us       1.84 GB           0 B           449  \n",
      "                                          aten::clone         0.06%       4.723ms         3.39%     290.057ms     623.779us       1.84 GB           0 B           465  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 8.569s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = profile_sample(116, trace_path=\"whisper_perfetto_large-v3_compiled.json\", model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92cf25-ea54-4bf2-bdb5-9e1ea99d0941",
   "metadata": {},
   "source": [
    "Видим ускорение. Однако, это может быть лаг неизолированной среды. Кроме того, эффективность torch.compile сильно зависит от warmup И длины последовательности. Сделаем тестовый прогон, чтобы увидеть динамику среднего времени выполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9282cdb6-2d8f-41d7-8d7c-311729106c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(verbose=False, model=None, dataset=None):\n",
    "    results = []\n",
    "    i = 0\n",
    "    for audio in tqdm(dataset):\n",
    "        audio_array = audio[\"audio\"][\"array\"]\n",
    "        sampling_rate = audio[\"audio\"][\"sampling_rate\"]\n",
    "        reference = audio[\"transcription\"]\n",
    "    \n",
    "        start_time = time.time()\n",
    "        input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features \n",
    "        predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)[0] #Уточнить в зависимости от выбранной модели\n",
    "        hypothesis = processor.decode(predicted_ids)\n",
    "        run_time = time.time() - start_time\n",
    "        metrics = asr_metrics(hypothesis, reference)\n",
    "        metrics[\"run_time_sec\"] = run_time\n",
    "        if verbose:\n",
    "            if i % 50 == 0:\n",
    "                print(\"referenct:\")\n",
    "                print(reference)\n",
    "                print(\"hypothesis:\")\n",
    "                print(hypothesis)\n",
    "            i += 1\n",
    "        results.append(metrics)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    summary = {\n",
    "        \"total_samples\": len(df_results),\n",
    "        \"avg_wer\": df_results[\"wer\"].mean(),\n",
    "        \"avg_cer\": df_results[\"cer\"].mean(),\n",
    "        \"avg_time_per_audio\": df_results[\"run_time_sec\"].mean(),\n",
    "        \"total_time\": df_results[\"run_time_sec\"].sum(),\n",
    "    }\n",
    "    \n",
    "    print(\"large-v3\")\n",
    "    print(json.dumps(summary, ensure_ascii=True, indent=2))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58cf44a1-10a3-4af3-933e-cebf88d068b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [06:50<00:00,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large-v3\n",
      "{\n",
      "  \"total_samples\": 50,\n",
      "  \"avg_wer\": 0.3984776334776335,\n",
      "  \"avg_cer\": 0.14465731826243972,\n",
      "  \"avg_time_per_audio\": 8.201239647865295,\n",
      "  \"total_time\": 410.06198239326477\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_small = dataset.select(range(50))\n",
    "_ = run_model(model=model, dataset=dataset_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "202f4a86-b6dc-4144-9489-1f897830e865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302245"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model, dataset_small, _\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2688b-c597-4504-b771-ecb4a493cdec",
   "metadata": {},
   "source": [
    "Это уже статистически значимый результат. Сбросим получившиеся ускорения и начнём квантизацию. Компиляцию применим после."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bae4f1-d9d6-4066-8e3c-6f73e76eff90",
   "metadata": {},
   "source": [
    "## PTQ Static\n",
    "Это неудачный эксперимент со статической квантизацией. Пояснения по шагам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc223fd9-c218-478b-9a1a-3eb3b8b7c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 6174.372281\n"
     ]
    }
   ],
   "source": [
    "qmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33abc5b-a60d-4c59-8bd4-52f514576a52",
   "metadata": {},
   "source": [
    "Было обнаружено, что подход \"в лоб\" не проходит из-за перенаправления вызова Conv1d внутри generate на CPU бэкэнд вместо QuantCPU совместимого, поэтому производится загрузка бэкэнда и работа только по Linear слоями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cbd28be-86d9-4b17-ac89-98ecf8a4a631",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperEncoderLayer(\n",
      "  (self_attn): WhisperAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3648411669.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.prepare(qmodel, inplace=True)\n",
      "/home/realn/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(\n",
       "            in_features=1280, out_features=5120, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=5120, out_features=1280, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(\n",
       "            in_features=1280, out_features=5120, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=5120, out_features=1280, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(\n",
       "    in_features=1280, out_features=51866, bias=False\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "qengine = torch.backends.quantized.engine\n",
    "qmodel.eval()\n",
    "\n",
    "def set_qconfig_for_linears(module):\n",
    "    if isinstance(module, (torch.nn.Linear)):\n",
    "        module.qconfig = tq.get_default_qconfig(qengine)\n",
    "\n",
    "qmodel.apply(set_qconfig_for_linears)\n",
    "\n",
    "print(qmodel.model.encoder.layers[0])\n",
    "\n",
    "tq.prepare(qmodel, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39b29423-c846-47a7-b566-db0be77d6a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current engine: fbgemm\n"
     ]
    }
   ],
   "source": [
    "print(\"current engine:\", torch.backends.quantized.engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a13333-a457-4f32-8684-b15b73d10676",
   "metadata": {},
   "source": [
    "Prepare -> Calibrate -> Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "505fd87b-da38-4d22-b488-312f0ddec3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                        | 0/10 [00:00<?, ?it/s]Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:56<00:00, 17.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# Калибровка\n",
    "qmodel.to(\"cpu\")\n",
    "\n",
    "dataset_smol = dataset.select(range(10))\n",
    "with torch.no_grad():\n",
    "    for i, audio in enumerate(tqdm(dataset_smol)):\n",
    "        audio_array = audio[\"audio\"][\"array\"]\n",
    "        sampling_rate = audio[\"audio\"][\"sampling_rate\"]\n",
    "        inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        _ = qmodel.generate(inputs.input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61d1c84d-781c-4263-a35f-6cf34dee79f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3360137250.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.convert(qmodel, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperEncoderLayer(\n",
      "  (self_attn): WhisperAttention(\n",
      "    (k_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.0764756053686142, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "    (v_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.04671403765678406, zero_point=69, qscheme=torch.per_channel_affine)\n",
      "    (q_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.07780876010656357, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (out_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.032819852232933044, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): QuantizedLinear(in_features=1280, out_features=5120, scale=0.10480756312608719, zero_point=49, qscheme=torch.per_channel_affine)\n",
      "  (fc2): QuantizedLinear(in_features=5120, out_features=1280, scale=0.029906874522566795, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Size (MB): 1852.556293\n"
     ]
    }
   ],
   "source": [
    "tq.convert(qmodel, inplace=True)\n",
    "print(qmodel.model.encoder.layers[0])\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da18898-ab77-40fd-9ad2-8f495cc77cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3000396167721783"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1852.556293/6174.372281"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa7b76-2f7f-47d4-939f-f0cb72b8edfd",
   "metadata": {},
   "source": [
    "### Замеры качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6cbecbf-3259-4e7d-8ac9-1fe4c53f8fd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                       | 0/793 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1603 [kernel]\nQuantizedCUDA: registered at /pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:387 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:115 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:468 [backend fallback]\nAutocastMAIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:506 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:544 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(verbose, model, dataset)\u001b[39m\n\u001b[32m      8\u001b[39m start_time = time.time()\n\u001b[32m      9\u001b[39m input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).input_features \n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m predicted_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m] \u001b[38;5;66;03m#Уточнить в зависимости от выбранной модели\u001b[39;00m\n\u001b[32m     11\u001b[39m hypothesis = processor.decode(predicted_ids)\n\u001b[32m     12\u001b[39m run_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:866\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001b[39m\n\u001b[32m    857\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    860\u001b[39m (\n\u001b[32m    861\u001b[39m     seek_sequences,\n\u001b[32m    862\u001b[39m     seek_outputs,\n\u001b[32m    863\u001b[39m     should_skip,\n\u001b[32m    864\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    865\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:1038\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m   1035\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m   1036\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2461\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[32m   2470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/generation/utils.py:861\u001b[39m, in \u001b[36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[39m\u001b[34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[39m\n\u001b[32m    859\u001b[39m encoder_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    860\u001b[39m encoder_kwargs[model_input_name] = inputs_tensor\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m]: ModelOutput = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:710\u001b[39m, in \u001b[36mWhisperEncoder.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    708\u001b[39m     layer_outputs = (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:411\u001b[39m, in \u001b[36mWhisperEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    409\u001b[39m residual = hidden_states\n\u001b[32m    410\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    418\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:316\u001b[39m, in \u001b[36mWhisperAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m q_input_shape = (bsz, tgt_len, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Scaling is susceptible to floating point arithmetics' inprecisions\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# which can lead to different results (this is dependent from model\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# to model, e.g. whisper is one such case). We therefore keep the\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# original order of scaling to follow the original implementation\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# and enforce no scaling (1.0) in the attention call below.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.scaling\n\u001b[32m    317\u001b[39m query_states = query_states.view(*q_input_shape)\n\u001b[32m    318\u001b[39m query_states = query_states.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:191\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/_ops.py:1255\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1603 [kernel]\nQuantizedCUDA: registered at /pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:387 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:115 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:468 [backend fallback]\nAutocastMAIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:506 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:544 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "results = run_model(model=qmodel, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7aa633-47e4-4a46-a29d-e106b2bff3a7",
   "metadata": {},
   "source": [
    " ### Почему происходит ошибка\n",
    "Во время вызова qmodel.generate(), когда Whisper создаёт временные тензоры в обычном CPU-бэкэнде, не переключённом в QuantizedCPU, создаётся обращение к слою LayerNorm.\n",
    "То есть слои QuantizedLayerNorm инициализированы правильно, но во время инференса их вызов идёт через AutogradCPU или Functionalize, а не через QuantizedCPU. При этом, переключение бэкэндом происходит автоматически и поэтому инференс, заданный таким образом, требует полной переработки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdf260-3f74-406d-8be6-f9d427f09947",
   "metadata": {},
   "source": [
    "Костыль номер 1 - инъекция, заменяющая работу forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93474aaa-02c8-44e8-81c9-ce36d4a03aba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import types\n",
    "from transformers.models.whisper.modeling_whisper import (\n",
    "    WhisperAttention,\n",
    "    WhisperEncoderLayer,\n",
    "    WhisperDecoderLayer,\n",
    ")\n",
    "\n",
    "def patch_whisper_for_quant(model):\n",
    "    \"\"\"\n",
    "    Патчит только WhisperAttention / WhisperEncoderLayer / WhisperDecoderLayer.\n",
    "    Именно там происходят вызовы F.linear и F.layer_norm.\n",
    "    \"\"\"\n",
    "\n",
    "    target_classes = (WhisperAttention, WhisperEncoderLayer, WhisperDecoderLayer)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if not isinstance(module, target_classes):\n",
    "            continue\n",
    "\n",
    "        if not hasattr(module, \"forward\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            src = inspect.getsource(module.forward)\n",
    "        except:\n",
    "            # Например, если source недоступен\n",
    "            continue\n",
    "\n",
    "        if \"F.linear\" not in src and \"F.layer_norm\" not in src:\n",
    "            continue\n",
    "\n",
    "        print(f\"[patching] {name}\")\n",
    "\n",
    "        patched = src\n",
    "        patched = patched.replace(\"F.linear\", \"self._patched_linear\")\n",
    "        patched = patched.replace(\"F.layer_norm\", \"self._patched_layer_norm\")\n",
    "\n",
    "        wrapper = f\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _patched_linear(self, x, weight, bias=None):\n",
    "    # input quantized?\n",
    "    if hasattr(x, 'is_quantized') and x.is_quantized:\n",
    "        try:\n",
    "            return torch.ops.quantized.linear(x, weight, bias)\n",
    "        except Exception:\n",
    "            return F.linear(x.dequantize(), weight, bias)\n",
    "    return F.linear(x, weight, bias)\n",
    "\n",
    "def _patched_layer_norm(self, x, normalized_shape, weight=None, bias=None, eps=1e-5):\n",
    "    if hasattr(x, 'is_quantized') and x.is_quantized:\n",
    "        try:\n",
    "            return torch.ops.quantized.layer_norm(x, normalized_shape, weight, bias, eps)\n",
    "        except Exception:\n",
    "            return F.layer_norm(x.dequantize(), normalized_shape, weight, bias, eps)\n",
    "    return F.layer_norm(x, normalized_shape, weight, bias, eps)\n",
    "\"\"\"\n",
    "\n",
    "        full_code = wrapper + \"\\n\" + patched\n",
    "\n",
    "        ctx = {\n",
    "            \"F\": F,\n",
    "            \"torch\": torch,\n",
    "        }\n",
    "\n",
    "        exec(full_code, ctx)\n",
    "        module.forward = types.MethodType(ctx[\"forward\"], module)\n",
    "\n",
    "    print(\"✅ Whisper F.linear / F.layer_norm patched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "990aff39-973c-4fd6-bcb4-c0fc99fff0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 6174.372281\n",
      "✅ Whisper F.linear / F.layer_norm patched.\n"
     ]
    }
   ],
   "source": [
    "# полностью перегрузим модель\n",
    "\n",
    "qmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "print_size_of_model(qmodel)\n",
    "\n",
    "patch_whisper_for_quant(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f41ba3f-7a1a-43aa-b9e6-f152ce6551cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperEncoderLayer(\n",
      "  (self_attn): WhisperAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3409767592.py:15: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.prepare(qmodel, inplace=True)\n",
      "/home/realn/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(\n",
       "            in_features=1280, out_features=5120, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=5120, out_features=1280, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(\n",
       "            in_features=1280, out_features=5120, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=5120, out_features=1280, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(\n",
       "    in_features=1280, out_features=51866, bias=False\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переквантуем\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "qengine = torch.backends.quantized.engine\n",
    "qmodel.eval()\n",
    "\n",
    "def set_qconfig_for_linears(module):\n",
    "    if isinstance(module, (torch.nn.Linear)):\n",
    "        module.qconfig = tq.get_default_qconfig(qengine)\n",
    "\n",
    "qmodel.apply(set_qconfig_for_linears)\n",
    "\n",
    "print(qmodel.model.encoder.layers[0])\n",
    "\n",
    "tq.prepare(qmodel, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0629c00-a915-495a-a6ca-6896d18babf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [03:00<00:00, 18.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Калибровка\n",
    "qmodel.to(\"cpu\")\n",
    "\n",
    "dataset_smol = dataset.select(range(10))\n",
    "with torch.no_grad():\n",
    "    for i, audio in enumerate(tqdm(dataset_smol)):\n",
    "        audio_array = audio[\"audio\"][\"array\"]\n",
    "        sampling_rate = audio[\"audio\"][\"sampling_rate\"]\n",
    "        inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        _ = qmodel.generate(inputs.input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13b118c1-d5c6-4c88-b327-9eb1ac1fcac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3360137250.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.convert(qmodel, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperEncoderLayer(\n",
      "  (self_attn): WhisperAttention(\n",
      "    (k_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.0764756053686142, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "    (v_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.04671403765678406, zero_point=69, qscheme=torch.per_channel_affine)\n",
      "    (q_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.07780876010656357, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (out_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.032819852232933044, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): QuantizedLinear(in_features=1280, out_features=5120, scale=0.10480756312608719, zero_point=49, qscheme=torch.per_channel_affine)\n",
      "  (fc2): QuantizedLinear(in_features=5120, out_features=1280, scale=0.029906874522566795, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Size (MB): 1852.556293\n"
     ]
    }
   ],
   "source": [
    "tq.convert(qmodel, inplace=True)\n",
    "print(qmodel.model.encoder.layers[0])\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e792de72-04c4-413b-9008-93fe119f1b6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1603 [kernel]\nQuantizedCUDA: registered at /pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:387 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:115 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:468 [backend fallback]\nAutocastMAIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:506 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:544 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m _ = \u001b[43mprofile_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m116\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhisper_perfetto_large-v3_quanted.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mprofile_sample\u001b[39m\u001b[34m(sample_idx, trace_path, sort_by, model)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profile(\n\u001b[32m      9\u001b[39m     activities=[ProfilerActivity.CPU],\n\u001b[32m     10\u001b[39m     record_shapes=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m     profile_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m     with_stack=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     13\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[33m\"\u001b[39m\u001b[33mwhisper.generate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         predicted_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m prof.export_chrome_trace(trace_path)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerfetto trace saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:866\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001b[39m\n\u001b[32m    857\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    860\u001b[39m (\n\u001b[32m    861\u001b[39m     seek_sequences,\n\u001b[32m    862\u001b[39m     seek_outputs,\n\u001b[32m    863\u001b[39m     should_skip,\n\u001b[32m    864\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    865\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:1038\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m   1035\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m   1036\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2461\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[32m   2470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/generation/utils.py:861\u001b[39m, in \u001b[36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[39m\u001b[34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[39m\n\u001b[32m    859\u001b[39m encoder_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    860\u001b[39m encoder_kwargs[model_input_name] = inputs_tensor\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m]: ModelOutput = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:710\u001b[39m, in \u001b[36mWhisperEncoder.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    708\u001b[39m     layer_outputs = (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:411\u001b[39m, in \u001b[36mWhisperEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    409\u001b[39m residual = hidden_states\n\u001b[32m    410\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    418\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:316\u001b[39m, in \u001b[36mWhisperAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m q_input_shape = (bsz, tgt_len, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Scaling is susceptible to floating point arithmetics' inprecisions\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# which can lead to different results (this is dependent from model\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# to model, e.g. whisper is one such case). We therefore keep the\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# original order of scaling to follow the original implementation\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# and enforce no scaling (1.0) in the attention call below.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.scaling\n\u001b[32m    317\u001b[39m query_states = query_states.view(*q_input_shape)\n\u001b[32m    318\u001b[39m query_states = query_states.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:191\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/_ops.py:1255\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1603 [kernel]\nQuantizedCUDA: registered at /pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:387 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:115 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:468 [backend fallback]\nAutocastMAIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:506 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:544 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "_ = profile_sample(116, trace_path=\"whisper_perfetto_large-v3_quanted.json\", model=qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481673a2-19d8-48b7-a462-307435b24a57",
   "metadata": {},
   "source": [
    "### Патч инъекция не сработала, попробуем другой костыль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef000bac-d077-42e6-9cb9-c0200ab7bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 6174.372281\n"
     ]
    }
   ],
   "source": [
    "qmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72aa998d-8ab3-4a57-9bc3-be035b327be6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperEncoderLayer(\n",
      "  (self_attn): WhisperAttention(\n",
      "    (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3409767592.py:15: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.prepare(qmodel, inplace=True)\n",
      "/home/realn/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(\n",
       "            in_features=1280, out_features=5120, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=5120, out_features=1280, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=False\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (v_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (q_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (out_proj): Linear(\n",
       "              in_features=1280, out_features=1280, bias=True\n",
       "              (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(\n",
       "            in_features=1280, out_features=5120, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (fc2): Linear(\n",
       "            in_features=5120, out_features=1280, bias=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(\n",
       "    in_features=1280, out_features=51866, bias=False\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переквантуем\n",
    "\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "qengine = torch.backends.quantized.engine\n",
    "qmodel.eval()\n",
    "\n",
    "def set_qconfig_for_linears(module):\n",
    "    if isinstance(module, (torch.nn.Linear)):\n",
    "        module.qconfig = tq.get_default_qconfig(qengine)\n",
    "\n",
    "qmodel.apply(set_qconfig_for_linears)\n",
    "\n",
    "print(qmodel.model.encoder.layers[0])\n",
    "\n",
    "tq.prepare(qmodel, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c6c025c-2de1-4365-8843-f935d9e72ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:58<00:00, 17.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# Калибровка\n",
    "qmodel.to(\"cpu\")\n",
    "\n",
    "dataset_smol = dataset.select(range(10))\n",
    "with torch.no_grad():\n",
    "    for i, audio in enumerate(tqdm(dataset_smol)):\n",
    "        audio_array = audio[\"audio\"][\"array\"]\n",
    "        sampling_rate = audio[\"audio\"][\"sampling_rate\"]\n",
    "        inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        _ = qmodel.generate(inputs.input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92a3870c-8159-4a24-a749-0d5213ed87fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3360137250.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.convert(qmodel, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperEncoderLayer(\n",
      "  (self_attn): WhisperAttention(\n",
      "    (k_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.0764756053686142, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "    (v_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.04671403765678406, zero_point=69, qscheme=torch.per_channel_affine)\n",
      "    (q_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.07780876010656357, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (out_proj): QuantizedLinear(in_features=1280, out_features=1280, scale=0.032819852232933044, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): QuantizedLinear(in_features=1280, out_features=5120, scale=0.10480756312608719, zero_point=49, qscheme=torch.per_channel_affine)\n",
      "  (fc2): QuantizedLinear(in_features=5120, out_features=1280, scale=0.029906874522566795, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "  (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Size (MB): 1852.556293\n"
     ]
    }
   ],
   "source": [
    "tq.convert(qmodel, inplace=True)\n",
    "print(qmodel.model.encoder.layers[0])\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534d32a-0a17-4415-975f-f5561e11d169",
   "metadata": {},
   "source": [
    "Сам костыль - мы хукаем F вызовы и заменяем на квантованные обработчики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6216250-fad0-4dca-a04a-ff56ac09c80c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Сохраним оригиналы\n",
    "_orig_F_linear = F.linear\n",
    "_orig_F_layer_norm = F.layer_norm\n",
    "\n",
    "# Вспомогательный контейнер для состояния\n",
    "_safe_quant_state = {\n",
    "    \"installed\": False,\n",
    "    \"model\": None,\n",
    "    \"orig_linear\": _orig_F_linear,\n",
    "    \"orig_layer_norm\": _orig_F_layer_norm,\n",
    "}\n",
    "\n",
    "def _find_quantized_linear_for_weight(model, weight_tensor):\n",
    "    \"\"\"\n",
    "    Ищем в model QuantizedLinear модуль, чей (распакованный/де-квантованный) вес\n",
    "    совпадает по форме с weight_tensor (best-effort).\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "\n",
    "    wt_shape = tuple(weight_tensor.shape) if hasattr(weight_tensor, \"shape\") else None\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        # Поддерживаем разные названия классов в разных версиях PyTorch\n",
    "        cls_name = type(m).__name__.lower()\n",
    "        if \"quantizedlinear\" in cls_name or \"quantizedlinear\" in cls_name.replace(\"_\", \"\") or \"quantizedlinear\" in cls_name.replace(\".\", \"\"):\n",
    "            # Попробуем получить вес в доступной форме\n",
    "            try:\n",
    "                w = getattr(m, \"weight\", None)\n",
    "                if isinstance(w, torch.Tensor):\n",
    "                    if tuple(w.shape) == wt_shape:\n",
    "                        return m\n",
    "                else:\n",
    "                    # У quantized modules weight() часто метод\n",
    "                    wcall = getattr(m, \"weight\", None)\n",
    "                    if callable(wcall):\n",
    "                        ww = wcall()\n",
    "                        if isinstance(ww, torch.Tensor) and tuple(ww.shape) == wt_shape:\n",
    "                            return m\n",
    "            except Exception:\n",
    "                # безопасно игнорируем ошибки доступа\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def _find_quantized_layernorm_for_shape(model, normalized_shape):\n",
    "    if model is None:\n",
    "        return None\n",
    "    target = tuple(normalized_shape) if hasattr(normalized_shape, \"__iter__\") else (int(normalized_shape),)\n",
    "    for name, m in model.named_modules():\n",
    "        cls_name = type(m).__name__.lower()\n",
    "        if \"quantizedlayernorm\" in cls_name or \"quantizedlayernorm\" in cls_name.replace(\"_\", \"\"):\n",
    "            try:\n",
    "                # пытаемся прочитать normalized_shape атрибут\n",
    "                ns = getattr(m, \"normalized_shape\", None)\n",
    "                if ns is None:\n",
    "                    ns = getattr(m, \"normalized_shape_\", None)\n",
    "                if ns is None and hasattr(m, \"_packed_params\"):\n",
    "                    # некоторые реализации хранят параметры иначе — пытаемся weight().shape\n",
    "                    w = getattr(m, \"weight\", None)\n",
    "                    if callable(w):\n",
    "                        w = w()\n",
    "                    if isinstance(w, torch.Tensor):\n",
    "                        ns = (w.shape[-1],)\n",
    "                if ns is not None and tuple(ns) == tuple(target):\n",
    "                    return m\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def _cached_find_quant_linear(model_id, wt_shape):\n",
    "    # model_id — id(model) to avoid capturing model in cache key incorrectly\n",
    "    # wt_shape — tuple\n",
    "    m = _safe_quant_state.get(\"model\", None)\n",
    "    if m is None:\n",
    "        return None\n",
    "    # Проходим по named_modules — используем helper\n",
    "    return _find_quantized_linear_for_weight(m, torch.empty(wt_shape))\n",
    "\n",
    "def install_safe_quant_inference(model):\n",
    "    \"\"\"\n",
    "    Устанавливает безопасные обёртки F.linear и F.layer_norm.\n",
    "    Перед вызовом generate сделай: install_safe_quant_inference(qmodel)\n",
    "    В конце можно вернуть оригинал: uninstall_safe_quant_inference()\n",
    "    \"\"\"\n",
    "    if _safe_quant_state[\"installed\"]:\n",
    "        return\n",
    "    _safe_quant_state[\"installed\"] = True\n",
    "    _safe_quant_state[\"model\"] = model\n",
    "\n",
    "    def _safe_linear(input, weight, bias=None):\n",
    "        # если вход не квантован — обычное поведение\n",
    "        if not (hasattr(input, \"is_quantized\") and input.is_quantized):\n",
    "            return _orig_F_linear(input, weight, bias)\n",
    "\n",
    "        # 1) Попытка вызвать нативный quantized op напрямую\n",
    "        try:\n",
    "            # Некоторые сборки ожидают определённый набор аргументов; пробуем наиболее общий вызов.\n",
    "            return torch.ops.quantized.linear(input, weight, bias)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 2) Поиск соответствующего QuantizedLinear в модели и вызов его forward\n",
    "        try:\n",
    "            # weight может быть Tensor; берем его shape\n",
    "            wt_shape = tuple(weight.shape) if hasattr(weight, \"shape\") else None\n",
    "            # Пытаться найти модуль\n",
    "            qmod = _find_quantized_linear_for_weight(_safe_quant_state.get(\"model\", None), weight)\n",
    "            if qmod is not None:\n",
    "                # При вызове модуля он сам разберётся с упаковкой весов\n",
    "                return qmod(input)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 3) Last resort: деквантовать input и вызвать оригинал\n",
    "        try:\n",
    "            deq = input.dequantize()\n",
    "            return _orig_F_linear(deq, weight, bias)\n",
    "        except Exception:\n",
    "            # В крайнем случае — пробуем оригинал и пусть поднимет ошибку\n",
    "            return _orig_F_linear(input, weight, bias)\n",
    "\n",
    "    def _safe_layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-5):\n",
    "        if not (hasattr(input, \"is_quantized\") and input.is_quantized):\n",
    "            return _orig_F_layer_norm(input, normalized_shape, weight, bias, eps)\n",
    "\n",
    "        # 1) Попытка вызвать нативный quantized op\n",
    "        try:\n",
    "            return torch.ops.quantized.layer_norm(input, normalized_shape, weight, bias, eps)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 2) Найти QuantizedLayerNorm модуль и вызвать его\n",
    "        try:\n",
    "            qln = _find_quantized_layernorm_for_shape(_safe_quant_state.get(\"model\", None), normalized_shape)\n",
    "            if qln is not None:\n",
    "                return qln(input)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 3) Fallback: деквантовать и запустить float path\n",
    "        try:\n",
    "            return _orig_F_layer_norm(input.dequantize(), normalized_shape, weight, bias, eps)\n",
    "        except Exception:\n",
    "            return _orig_F_layer_norm(input, normalized_shape, weight, bias, eps)\n",
    "\n",
    "    # Установим патчи\n",
    "    F.linear = _safe_linear\n",
    "    F.layer_norm = _safe_layer_norm\n",
    "\n",
    "def uninstall_safe_quant_inference():\n",
    "    \"\"\"Восстановить исходные F.linear и F.layer_norm.\"\"\"\n",
    "    if not _safe_quant_state[\"installed\"]:\n",
    "        return\n",
    "    F.linear = _safe_quant_state[\"orig_linear\"]\n",
    "    F.layer_norm = _safe_quant_state[\"orig_layer_norm\"]\n",
    "    _safe_quant_state[\"installed\"] = False\n",
    "    _safe_quant_state[\"model\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7af19de5-2039-44e3-8006-2cd3983a78d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1603 [kernel]\nQuantizedCUDA: registered at /pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:387 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:115 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:468 [backend fallback]\nAutocastMAIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:506 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:544 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[33m\"\u001b[39m\u001b[33mwhisper.generate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     15\u001b[39m         install_safe_quant_inference(qmodel)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         predicted_ids = \u001b[43mqmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m prof.export_chrome_trace(trace_path)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerfetto trace saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:866\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, monitor_progress, **kwargs)\u001b[39m\n\u001b[32m    857\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    860\u001b[39m (\n\u001b[32m    861\u001b[39m     seek_sequences,\n\u001b[32m    862\u001b[39m     seek_outputs,\n\u001b[32m    863\u001b[39m     should_skip,\n\u001b[32m    864\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    865\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:1038\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, seek, batch_idx_map, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m   1035\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m   1036\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2461\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[32m   2470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/generation/utils.py:861\u001b[39m, in \u001b[36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[39m\u001b[34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[39m\n\u001b[32m    859\u001b[39m encoder_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    860\u001b[39m encoder_kwargs[model_input_name] = inputs_tensor\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m]: ModelOutput = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:710\u001b[39m, in \u001b[36mWhisperEncoder.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    708\u001b[39m     layer_outputs = (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:411\u001b[39m, in \u001b[36mWhisperEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    409\u001b[39m residual = hidden_states\n\u001b[32m    410\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    418\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:316\u001b[39m, in \u001b[36mWhisperAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m q_input_shape = (bsz, tgt_len, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# Scaling is susceptible to floating point arithmetics' inprecisions\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# which can lead to different results (this is dependent from model\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# to model, e.g. whisper is one such case). We therefore keep the\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# original order of scaling to follow the original implementation\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# and enforce no scaling (1.0) in the attention call below.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.scaling\n\u001b[32m    317\u001b[39m query_states = query_states.view(*q_input_shape)\n\u001b[32m    318\u001b[39m query_states = query_states.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:191\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_point\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/torch/_ops.py:1255\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at /pytorch/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1603 [kernel]\nQuantizedCUDA: registered at /pytorch/aten/src/ATen/native/quantized/cudnn/Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:387 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:115 [backend fallback]\nADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nAutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:468 [backend fallback]\nAutocastMAIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:506 [backend fallback]\nAutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:544 [backend fallback]\nAutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "trace_path=\"whisper_perfetto_large-v3_quanted.json\"\n",
    "example = dataset[116]\n",
    "audio_array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "\n",
    "with profile(\n",
    "        activities=[ProfilerActivity.CPU],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "    with record_function(\"whisper.generate\"):\n",
    "        install_safe_quant_inference(qmodel)\n",
    "        predicted_ids = qmodel.generate(inputs, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "prof.export_chrome_trace(trace_path)\n",
    "print(f\"Perfetto trace saved to {trace_path}\")\n",
    "print(prof.key_averages().table(\n",
    "        sort_by=sort_by,\n",
    "        row_limit=10\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f5fd7-1a83-4370-abd6-d1b7fbc8c21f",
   "metadata": {},
   "source": [
    "Тоже не сработала.\n",
    "\n",
    "Вывод - текущими средствами без изменения архитектуры модели (т.е. замены внутри forward модели обращения к nn.F блокам на nn блоки) статическую квантизацию сделать не получается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f313304-01c9-4ef5-8ec2-93ca14e2b4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63116"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del example, audio_array, sampling_rate, inputs, prof, qmodel\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9c167-a16f-4c34-b1c3-8dde9595533a",
   "metadata": {},
   "source": [
    "# PTQ Dynamic\n",
    "Простейшая восьмибитная квантизация в одну строчку. 8 бит были выбраны не случайно, т.к. по графикам с лекций после 8 бит драматически снижается качество модели, а ASR модели чувствительны к вычислительной точности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe337252-49dd-44c9-8f6e-a38b68287cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 6174.372281\n"
     ]
    }
   ],
   "source": [
    "qmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b548bcbb-4bab-4998-9e40-438568842097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48834/3875406213.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qmodel = tq.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 1837.108365\n"
     ]
    }
   ],
   "source": [
    "modules_to_quantize = {torch.nn.Linear}\n",
    "qmodel = tq.quantize_dynamic(\n",
    "    qmodel, \n",
    "    modules_to_quantize, \n",
    "    dtype=torch.qint8\n",
    ")\n",
    "print_size_of_model(qmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a02b80e-9abc-42dd-904b-37bcf1c03af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_quanted.json\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     whisper.generate         7.86%     704.158ms       100.00%        8.964s        8.964s         112 B      -6.85 GB             1  \n",
      "                            quantized::linear_dynamic        48.54%        4.351s        49.63%        4.449s       1.082ms       2.55 GB      -2.55 GB          4111  \n",
      "                   aten::scaled_dot_product_attention         0.18%      16.361ms        16.09%        1.443s       1.454ms     240.00 MB      -3.75 MB           992  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu        14.96%        1.341s        15.91%        1.426s       1.438ms     243.75 MB    -155.99 MB           992  \n",
      "                                            aten::cat         7.68%     688.180ms         7.73%     693.211ms     655.829us     527.72 MB     527.72 MB          1057  \n",
      "                                     aten::contiguous         0.02%       1.705ms         5.34%     478.262ms       1.065ms       1.84 GB           0 B           449  \n",
      "                                          aten::clone         0.06%       5.803ms         5.32%     476.797ms       1.025ms       1.84 GB           0 B           465  \n",
      "                                          aten::copy_         5.27%     472.768ms         5.27%     472.768ms     286.874us           0 B           0 B          1648  \n",
      "                                           aten::isin         0.11%       9.632ms         3.35%     299.915ms       6.520ms       1.48 MB     -31.21 MB            46  \n",
      "                                     aten::layer_norm         0.09%       8.027ms         3.19%     285.880ms     188.079us     484.60 MB    -774.70 KB          1520  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 8.964s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = profile_sample(116, trace_path=\"whisper_perfetto_large-v3_quanted.json\", model=qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8bf6d-3a00-410a-9f86-448027abd0cb",
   "metadata": {},
   "source": [
    "Мы получили значительное ускорение инференса. Попробуем снять замеры качества с учётом torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bc0ce6c-adf0-49b5-bb9a-ff8acc037c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_quanted_compiled.json\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     whisper.generate         8.22%     634.960ms       100.00%        7.726s        7.726s         112 B      -6.85 GB             1  \n",
      "                            quantized::linear_dynamic        48.54%        3.750s        49.43%        3.819s     929.023us       2.55 GB      -2.55 GB          4111  \n",
      "                   aten::scaled_dot_product_attention         0.10%       7.414ms        16.53%        1.277s       1.288ms     240.00 MB      -3.75 MB           992  \n",
      "    aten::_scaled_dot_product_flash_attention_for_cpu        15.84%        1.223s        16.44%        1.270s       1.280ms     243.75 MB    -155.99 MB           992  \n",
      "                                     aten::contiguous         0.02%       1.767ms        11.46%     885.782ms       1.973ms       1.84 GB           0 B           449  \n",
      "                                          aten::clone         0.11%       8.360ms        11.44%     884.227ms       1.902ms       1.84 GB           0 B           465  \n",
      "                                          aten::copy_        11.32%     874.642ms        11.32%     874.642ms     530.729us           0 B           0 B          1648  \n",
      "                                            aten::cat         5.02%     388.120ms         5.06%     390.632ms     369.567us     527.72 MB     527.72 MB          1057  \n",
      "                                     aten::layer_norm         0.10%       7.417ms         3.06%     236.377ms     155.511us     484.60 MB    -774.69 KB          1520  \n",
      "                              aten::native_layer_norm         0.96%      73.789ms         2.96%     228.960ms     150.632us     485.36 MB    -476.07 MB          1520  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.726s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel = torch.compile(qmodel)\n",
    "_ = profile_sample(116, trace_path=\"whisper_perfetto_large-v3_quanted_compiled.json\", model=qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f043081-02a2-4a83-bc48-7286f73b5771",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                                                                            | 1/793 [00:06<1:27:06,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "можешь включить сериал теория большого взрыва\n",
      "hypothesis:\n",
      " Можешь включить сериал «Теория большого взрыва»?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████████████                                                                                                                                                                                               | 51/793 [04:59<1:16:07,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "покажи на смотрешке канал бридж тв\n",
      "hypothesis:\n",
      " Покажи на сматрёшке канал Бридж ТВ.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████████████████▊                                                                                                                                                                                 | 101/793 [09:55<1:03:30,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "асият иванов\n",
      "hypothesis:\n",
      " Асиат Иванов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████████████████████████████████████▋                                                                                                                                                                    | 151/793 [14:45<1:04:01,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "заказать тольятти молоко три и два процента жирности один литр\n",
      "hypothesis:\n",
      " Заказать в Тольятти молоко 3,2% жирности 1 литр.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████████████████████████▉                                                                                                                                                         | 201/793 [19:34<57:09,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "фильм самый лучший день\n",
      "hypothesis:\n",
      " Фильм «Самый лучший день»\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████████████████████████████████████████▉                                                                                                                                            | 251/793 [24:21<49:43,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "лилль\n",
      "hypothesis:\n",
      " Лиль\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████████████████████████████████████████████████████▊                                                                                                                               | 301/793 [29:07<46:57,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "брюс уиллис\n",
      "hypothesis:\n",
      " Брюс Уиллис\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                  | 351/793 [34:07<42:57,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "ооо грузовой легковой шиномонтаж\n",
      "hypothesis:\n",
      " О-о-о, грузовой легковой шиномонтаж.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                     | 401/793 [39:07<39:35,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "покажи мне амирана сардарова на ютюбе\n",
      "hypothesis:\n",
      " Покажи мне Амирана Сардарова на YouTube.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                        | 451/793 [43:56<35:12,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "арсенал манчестер сити\n",
      "hypothesis:\n",
      " Арсенал Манчестер Сити\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                           | 501/793 [48:49<31:09,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "у тебя в каталоге есть сериал охотники за бриллиантами первый сезон\n",
      "hypothesis:\n",
      " У тебя в каталоге есть сериал «Охотники за бриллиантами. Первый сезон».\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                              | 551/793 [53:35<22:31,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "джой сколько страниц в собака баскервилей\n",
      "hypothesis:\n",
      " Джой, сколько страниц в собак обоскервилий?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 601/793 [58:23<17:20,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "шант ньюс\n",
      "hypothesis:\n",
      " Шант Ньюс\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                    | 651/793 [1:03:06<13:06,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "танго любви найди\n",
      "hypothesis:\n",
      " Танго любви найди.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 701/793 [1:07:56<08:47,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "вячеслав владимирович месяцев\n",
      "hypothesis:\n",
      " Вячеслав Владимирович Месяцев\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 751/793 [1:12:49<03:59,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "referenct:\n",
      "футбольный матч тоттенхэм лестер\n",
      "hypothesis:\n",
      " Футбольный матч Тоттенхэм-Лестер\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 793/793 [1:16:50<00:00,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large-v3\n",
      "{\n",
      "  \"total_samples\": 793,\n",
      "  \"avg_wer\": 0.4739530218975364,\n",
      "  \"avg_cer\": 0.16637995920858634,\n",
      "  \"avg_time_per_audio\": 5.805704870861385,\n",
      "  \"total_time\": 4603.923962593079\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary = run_model(verbose=True, model=qmodel, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66f3155b-d6e9-4d88-ad25-3991791dbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"whisper_metric.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[\"large-v3_cpu_quanted\"] = summary\n",
    "\n",
    "with open(\"whisper_metric.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "275ae158-40db-4c99-9b23-2cece29e4235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tiny</th>\n",
       "      <th>small</th>\n",
       "      <th>iarge-v3</th>\n",
       "      <th>large-v3_cuda</th>\n",
       "      <th>large-v3_cpu_quanted</th>\n",
       "      <th>large-v3_cuda_quanted</th>\n",
       "      <th>large-v3_cuda_autocast_compile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_samples</th>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_wer</th>\n",
       "      <td>1.049771</td>\n",
       "      <td>0.523011</td>\n",
       "      <td>0.440303</td>\n",
       "      <td>0.440303</td>\n",
       "      <td>0.473953</td>\n",
       "      <td>0.447231</td>\n",
       "      <td>0.442587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cer</th>\n",
       "      <td>0.486142</td>\n",
       "      <td>0.207796</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.166380</td>\n",
       "      <td>0.158552</td>\n",
       "      <td>0.158944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_time_per_audio</th>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.682818</td>\n",
       "      <td>7.948655</td>\n",
       "      <td>0.838627</td>\n",
       "      <td>5.805705</td>\n",
       "      <td>2.843514</td>\n",
       "      <td>0.997778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_time</th>\n",
       "      <td>133.937801</td>\n",
       "      <td>541.474906</td>\n",
       "      <td>6303.283459</td>\n",
       "      <td>665.031494</td>\n",
       "      <td>4603.923963</td>\n",
       "      <td>2254.906898</td>\n",
       "      <td>791.237885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          tiny       small     iarge-v3  large-v3_cuda  \\\n",
       "total_samples       793.000000  793.000000   793.000000     793.000000   \n",
       "avg_wer               1.049771    0.523011     0.440303       0.440303   \n",
       "avg_cer               0.486142    0.207796     0.158293       0.158293   \n",
       "avg_time_per_audio    0.168900    0.682818     7.948655       0.838627   \n",
       "total_time          133.937801  541.474906  6303.283459     665.031494   \n",
       "\n",
       "                    large-v3_cpu_quanted  large-v3_cuda_quanted  \\\n",
       "total_samples                 793.000000             793.000000   \n",
       "avg_wer                         0.473953               0.447231   \n",
       "avg_cer                         0.166380               0.158552   \n",
       "avg_time_per_audio              5.805705               2.843514   \n",
       "total_time                   4603.923963            2254.906898   \n",
       "\n",
       "                    large-v3_cuda_autocast_compile  \n",
       "total_samples                           793.000000  \n",
       "avg_wer                                   0.442587  \n",
       "avg_cer                                   0.158944  \n",
       "avg_time_per_audio                        0.997778  \n",
       "total_time                              791.237885  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a325f2a-0389-4d78-b0f3-0e06151267f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем state_dict квантованной модели\n",
    "torch.save(qmodel.state_dict(), \"./whisper-large-v3-quantized-dynamic.pth\")\n",
    "\n",
    "# Также сохраните конфигурацию отдельно (она не меняется)\n",
    "qmodel.config.save_pretrained(\"./whisper-large-v3-quantized-dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4678e7c-6d0a-43e2-9fc3-915e7a5d2540",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mqmodel\u001b[49m, _\n\u001b[32m      2\u001b[39m gc.collect()\n",
      "\u001b[31mNameError\u001b[39m: name 'qmodel' is not defined"
     ]
    }
   ],
   "source": [
    "del qmodel, _\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf24afa-85cc-4369-a0a1-e98b02c0f544",
   "metadata": {},
   "source": [
    "# CUDA\n",
    "В CUDA нет наивной реализации квантизации и требуется самостоятельно писать необходимые операции.\n",
    "Однако, при работе с трансформерами можно ожидать готового решения для каждой конкретной модели. В нашем случае существует минимум три готовых варианта:\n",
    "1. TensorRT ускорение с помощью смешанной точности. Не завёлся, т.к. требует CUDA>= 12.9, а переставлять тулкиты = ломать текущие зависимости проекта, что весьма накладно. Основная идея - работать в half_precision с INT8/FP16 fallback\n",
    "2. Квантизация через BitsAndBytes backend. Будем пробовать.\n",
    "3. FP8/SmoothQuant через TransformerEngine. Требует видеокарты архитектуры Hopper/Ada, доступа к такой нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e67168d8-37f9-41ab-b87e-459e603a1ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92dad060-f5c0-467e-b50f-1142ba75d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  4 23:19:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 572.70         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "| 60%   30C    P8             11W /  285W |    1167MiB /  12282MiB |      7%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A              33      G   /Xwayland                             N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc06e6c0-aff9-4c92-afe6-44ba26cb009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 1623.253273\n"
     ]
    }
   ],
   "source": [
    "qmodel_cuda = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\", \n",
    "                                                              load_in_8bit=True,   # или load_in_4bit=True\n",
    "                                                              device_map=\"auto\")\n",
    "print_size_of_model(qmodel_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8f4fef1-bb20-44c4-b8f6-9ea0f4b826f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_cuda_quantized.json\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       whisper.generate         0.00%       0.000us         0.00%       0.000us       0.000us        4.442s       956.87%        4.442s        4.442s           0 B           0 B           0 B           0 B             1  \n",
      "                                       whisper.generate        10.94%     486.627ms        99.99%        4.447s        4.447s       0.000us         0.00%     464.239ms     464.239ms           0 B        -903 B      64.50 KB      -3.55 GB             1  \n",
      "                                           MatMul8bitLt         7.38%     328.412ms        75.38%        3.353s     818.589us       0.000us         0.00%     315.846ms      77.111us           0 B           0 B       1.27 GB    -758.13 MB          4096  \n",
      "                     bitsandbytes::int8_mixed_scaled_mm         3.76%     167.102ms        28.05%        1.248s     304.641us       0.000us         0.00%     208.847ms      50.988us           0 B           0 B       1.28 GB    -605.98 MB          4096  \n",
      "                           bitsandbytes::int8_scaled_mm         3.68%     163.565ms        21.97%     977.064ms     238.541us       0.000us         0.00%     202.219ms      49.370us           0 B           0 B       1.27 GB      -2.55 GB          4096  \n",
      "                       bitsandbytes::int8_linear_matmul         6.12%     272.133ms         9.11%     405.350ms      98.962us     177.537ms        38.24%     177.537ms      43.344us           0 B           0 B       2.55 GB           0 B          4096  \n",
      "void gemmSN_kernel_int32<256, 32, 4, 8, 8, 1, true, ...         0.00%       0.000us         0.00%       0.000us       0.000us     108.717ms        23.42%     108.717ms      30.334us           0 B           0 B           0 B           0 B          3584  \n",
      "                    bitsandbytes::int8_vectorwise_quant        10.02%     445.516ms        38.65%        1.719s     419.753us      36.942ms         7.96%     107.020ms      26.128us           0 B           0 B     755.15 MB      -1.91 GB          4096  \n",
      "                     aten::scaled_dot_product_attention         0.38%      16.862ms         3.27%     145.555ms     146.729us       0.000us         0.00%      63.902ms      64.417us           0 B           0 B     120.00 MB      -5.11 MB           992  \n",
      "              aten::_scaled_dot_product_flash_attention         0.42%      18.718ms         2.89%     128.693ms     129.730us       0.000us         0.00%      63.902ms      64.417us           0 B           0 B     125.11 MB           0 B           992  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.448s\n",
      "Self CUDA time total: 464.227ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel_cuda.eval()\n",
    "qmodel_cuda = torch.compile(qmodel_cuda)\n",
    "\n",
    "def profile_sample_cuda(sample_idx=0, trace_path=\"whisper_perfetto_large-v3_cuda.json\", model=None):\n",
    "    example = dataset[sample_idx]\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    inputs = inputs.half().to(device)  \n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],  \n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "        with record_function(\"whisper.generate\"):\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(inputs, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "    prof.export_chrome_trace(trace_path)\n",
    "    print(f\"Perfetto trace saved to {trace_path}\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"cuda_time_total\",\n",
    "        row_limit=10\n",
    "    ))\n",
    "    return processor.decode(predicted_ids[0].cpu())\n",
    "\n",
    "_ = profile_sample_cuda(116, trace_path=\"whisper_perfetto_large-v3_cuda_quantized.json\", model=qmodel_cuda)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d5f515a-457c-4df2-8d93-7f7f208dd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_cuda_quant(verbose=False, model=None, dataset=None):\n",
    "    results = []\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for audio in tqdm(dataset):\n",
    "            audio_array = audio[\"audio\"][\"array\"]\n",
    "            sampling_rate = audio[\"audio\"][\"sampling_rate\"]\n",
    "            reference = audio[\"transcription\"]\n",
    "        \n",
    "            start_time = time.time()\n",
    "            input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features \n",
    "            input_features = input_features.half().to(device)\n",
    "            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)[0] #Уточнить в зависимости от выбранной модели\n",
    "            hypothesis = processor.decode(predicted_ids)\n",
    "            run_time = time.time() - start_time\n",
    "            metrics = asr_metrics(hypothesis, reference)\n",
    "            metrics[\"run_time_sec\"] = run_time\n",
    "            if verbose:\n",
    "                if i % 50 == 0:\n",
    "                    print(\"referenct:\")\n",
    "                    print(reference)\n",
    "                    print(\"hypothesis:\")\n",
    "                    print(hypothesis)\n",
    "                i += 1\n",
    "            results.append(metrics)\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    summary = {\n",
    "        \"total_samples\": len(df_results),\n",
    "        \"avg_wer\": df_results[\"wer\"].mean(),\n",
    "        \"avg_cer\": df_results[\"cer\"].mean(),\n",
    "        \"avg_time_per_audio\": df_results[\"run_time_sec\"].mean(),\n",
    "        \"total_time\": df_results[\"run_time_sec\"].sum(),\n",
    "    }\n",
    "    \n",
    "    print(\"large-v3_cuda_q\")\n",
    "    print(json.dumps(summary, ensure_ascii=True, indent=2))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1287a336-8c43-4fbe-9689-5ec4ea48cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 793/793 [39:51<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large-v3_cuda_q\n",
      "{\n",
      "  \"total_samples\": 793,\n",
      "  \"avg_wer\": 0.44723060042101653,\n",
      "  \"avg_cer\": 0.1585522291109167,\n",
      "  \"avg_time_per_audio\": 3.006850938327977,\n",
      "  \"total_time\": 2384.4327940940857\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary = run_model_cuda_quant(model=qmodel_cuda, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bbdf277-a7a0-48e4-9dea-0c636d2869fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"large-v3_cuda_quanted\"] = summary\n",
    "\n",
    "with open(\"whisper_metric.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9492aa79-7326-450e-8e9a-df51d3a88878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tiny</th>\n",
       "      <th>small</th>\n",
       "      <th>iarge-v3</th>\n",
       "      <th>large-v3_cuda</th>\n",
       "      <th>large-v3_cpu_quanted</th>\n",
       "      <th>large-v3_cuda_quanted</th>\n",
       "      <th>large-v3_cuda_autocast_compile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_samples</th>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_wer</th>\n",
       "      <td>1.049771</td>\n",
       "      <td>0.523011</td>\n",
       "      <td>0.440303</td>\n",
       "      <td>0.440303</td>\n",
       "      <td>0.473953</td>\n",
       "      <td>0.447231</td>\n",
       "      <td>0.442587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cer</th>\n",
       "      <td>0.486142</td>\n",
       "      <td>0.207796</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.166380</td>\n",
       "      <td>0.158552</td>\n",
       "      <td>0.158944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_time_per_audio</th>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.682818</td>\n",
       "      <td>7.948655</td>\n",
       "      <td>0.838627</td>\n",
       "      <td>5.805705</td>\n",
       "      <td>3.006851</td>\n",
       "      <td>0.997778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_time</th>\n",
       "      <td>133.937801</td>\n",
       "      <td>541.474906</td>\n",
       "      <td>6303.283459</td>\n",
       "      <td>665.031494</td>\n",
       "      <td>4603.923963</td>\n",
       "      <td>2384.432794</td>\n",
       "      <td>791.237885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          tiny       small     iarge-v3  large-v3_cuda  \\\n",
       "total_samples       793.000000  793.000000   793.000000     793.000000   \n",
       "avg_wer               1.049771    0.523011     0.440303       0.440303   \n",
       "avg_cer               0.486142    0.207796     0.158293       0.158293   \n",
       "avg_time_per_audio    0.168900    0.682818     7.948655       0.838627   \n",
       "total_time          133.937801  541.474906  6303.283459     665.031494   \n",
       "\n",
       "                    large-v3_cpu_quanted  large-v3_cuda_quanted  \\\n",
       "total_samples                 793.000000             793.000000   \n",
       "avg_wer                         0.473953               0.447231   \n",
       "avg_cer                         0.166380               0.158552   \n",
       "avg_time_per_audio              5.805705               3.006851   \n",
       "total_time                   4603.923963            2384.432794   \n",
       "\n",
       "                    large-v3_cuda_autocast_compile  \n",
       "total_samples                           793.000000  \n",
       "avg_wer                                   0.442587  \n",
       "avg_cer                                   0.158944  \n",
       "avg_time_per_audio                        0.997778  \n",
       "total_time                              791.237885  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b89eae11-4b6f-4d06-b9df-290a22bccbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del qmodel_cuda, _\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467e902-039e-4402-a6eb-bf42788c1adf",
   "metadata": {},
   "source": [
    "# The old way. Quantization vs Autocast\n",
    "\n",
    "Во времена, когда деревья были большими, а ML-инженеры - тупенькими, использовался проверенный бородатыми сеньорами способ torch.compile + torch.autocast.\n",
    "\n",
    "Пришло время проверить эту гипотезу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072556b8-8f89-450c-a587-c498fa72faaa",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19f8eba7-c0a6-4fc1-9e10-4677e492ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7bacb315-5099-4c09-9d4a-13bc61643946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_cpu_autocasted.json\n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                         aten::conv1d         0.00%     891.711us       105.30%       91.128s       22.782s      32.29 MB    -750.00 KB             4  \n",
      "                                     whisper.generate         0.58%     503.662ms        99.99%       86.532s       86.532s         112 B      -6.78 GB             1  \n",
      "                                         aten::linear         0.17%     143.439ms        77.77%       67.304s       9.785ms       5.41 GB    -710.20 MB          6878  \n",
      "                                    aten::convolution         0.00%      31.884us        52.65%       45.561s       22.780s      10.99 MB           0 B             2  \n",
      "                                   aten::_convolution         0.01%      10.463ms        52.65%       45.561s       22.780s      10.99 MB           0 B             2  \n",
      "                                    aten::thnn_conv2d         0.00%       3.047ms        52.63%       45.549s       22.775s      10.99 MB           0 B             2  \n",
      "                           aten::_slow_conv2d_forward        52.59%       45.512s        52.63%       45.546s       22.773s      10.99 MB     -13.18 MB             2  \n",
      "                                          aten::addmm        28.80%       24.922s        30.60%       26.482s       7.455ms       1.04 GB       1.04 GB          3552  \n",
      "                                          aten::copy_         6.81%        5.895s         6.81%        5.895s     575.260us           0 B           0 B         10248  \n",
      "                                         aten::matmul         0.02%      16.077ms         5.79%        5.014s       8.969ms     237.56 MB           0 B           559  \n",
      "-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 86.540s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_a.eval()\n",
    "model_a = torch.compile(model_a)\n",
    "\n",
    "def profile_sample_autocast(sample_idx=0, trace_path=\"whisper_perfetto_large-v3_cpu_a.json\", model=None, device='cpu'):\n",
    "    example = dataset[sample_idx]\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    inputs = inputs.to(device)  \n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],  \n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "        with record_function(\"whisper.generate\"):\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    predicted_ids = model.generate(inputs, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "    prof.export_chrome_trace(trace_path)\n",
    "    print(f\"Perfetto trace saved to {trace_path}\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"cpu_time_total\",\n",
    "        row_limit=10\n",
    "    ))\n",
    "    return processor.decode(predicted_ids[0].cpu())\n",
    "\n",
    "_ = profile_sample_autocast(116, trace_path=\"whisper_perfetto_large-v3_cpu_autocasted.json\", model=model_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb44085-5f88-4eb8-9ac1-3b73c62c183e",
   "metadata": {},
   "source": [
    "### CUDA raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98c59631-aa69-4cd8-9de7-87e8f2254c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model_a.to(device).eval()\n",
    "model_a = torch.compile(model_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87404de7-6ced-42a6-b077-d4ca967de9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_CUDA_autocasted.json\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       whisper.generate         0.00%       0.000us         0.00%       0.000us       0.000us        3.019s      2040.76%        3.019s        3.019s           0 B           0 B           0 B           0 B             1  \n",
      "                                           aten::linear         3.18%     133.022ms        66.31%        2.774s     403.283us       0.000us         0.00%     159.110ms      23.133us           0 B           0 B       5.51 GB    -695.73 MB          6878  \n",
      "                                       whisper.generate        11.50%     481.059ms        99.99%        4.183s        4.183s       0.000us         0.00%     148.470ms     148.470ms           0 B        -887 B       1.00 MB      -6.88 GB             1  \n",
      "                                            aten::addmm        10.62%     444.318ms        21.76%     910.134ms     256.231us      65.257ms        44.12%      65.378ms      18.406us           0 B           0 B       1.04 GB       1.04 GB          3552  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      31.451ms        21.26%      31.451ms      34.562us           0 B           0 B           0 B           0 B           910  \n",
      "                                            aten::copy_         1.36%      56.884ms         4.30%     179.693ms      41.877us      29.719ms        20.09%      29.719ms       6.926us           0 B           0 B           0 B           0 B          4291  \n",
      "                                               aten::to         0.33%      13.925ms        13.69%     572.694ms     152.029us       0.000us         0.00%      26.484ms       7.030us          30 B           0 B       3.65 GB           0 B          3767  \n",
      "                                         aten::_to_copy         0.93%      38.735ms        13.36%     558.769ms     151.059us       0.000us         0.00%      26.484ms       7.160us          30 B           0 B       3.65 GB           0 B          3699  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      26.450ms        17.88%      26.450ms       7.996us           0 B           0 B           0 B           0 B          3308  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      25.664ms        17.35%      25.664ms       9.548us           0 B           0 B           0 B           0 B          2688  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.183s\n",
      "Self CUDA time total: 147.924ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def profile_sample_autocast_cuda(sample_idx=0, trace_path=\"whisper_perfetto_large-v3_cuda.json\", model=None, device='cpu'):\n",
    "    example = dataset[sample_idx]\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    inputs = inputs.half().to(device)  \n",
    "\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],  \n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=False,\n",
    "    ) as prof:\n",
    "        with record_function(\"whisper.generate\"):\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    predicted_ids = model.generate(inputs, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "    prof.export_chrome_trace(trace_path)\n",
    "    print(f\"Perfetto trace saved to {trace_path}\")\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"cuda_time_total\",\n",
    "        row_limit=10\n",
    "    ))\n",
    "    return processor.decode(predicted_ids[0].cpu())\n",
    "\n",
    "_ = profile_sample_autocast_cuda(116, trace_path=\"whisper_perfetto_large-v3_CUDA_autocasted.json\",  model=model_a, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc181d8e-fad6-4bc5-9682-77d4bd41905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del _\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae48f325-c57c-450e-a85d-e02e37d12b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 793/793 [11:58<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for audio in tqdm(dataset):\n",
    "        audio_array = audio[\"audio\"][\"array\"]\n",
    "        sampling_rate = audio[\"audio\"][\"sampling_rate\"]\n",
    "        reference = audio[\"transcription\"]\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "        input_features = input_features.half().to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            predicted_ids = model_a.generate(input_features, forced_decoder_ids=forced_decoder_ids)[0]\n",
    "        \n",
    "        \n",
    "        hypothesis = processor.decode(predicted_ids.cpu())\n",
    "        \n",
    "        run_time = time.time() - start_time\n",
    "        metrics = asr_metrics(hypothesis, reference)\n",
    "        metrics[\"run_time_sec\"] = run_time\n",
    "        \n",
    "        # if i % 50 == 0:\n",
    "        #     print(\"reference:\")\n",
    "        #     print(reference)\n",
    "        #     print(\"hypothesis:\")\n",
    "        #     print(hypothesis)\n",
    "        # i += 1\n",
    "        \n",
    "        results.append(metrics)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c673b631-dc44-4562-8df3-45858d0c0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large-v3_cuda_q\n",
      "{\n",
      "  \"total_samples\": 793,\n",
      "  \"avg_wer\": 0.44258729290885657,\n",
      "  \"avg_cer\": 0.15894431408815926,\n",
      "  \"avg_time_per_audio\": 0.8967222247418359,\n",
      "  \"total_time\": 711.1007242202759\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "summary = {\n",
    "    \"total_samples\": len(df_results),\n",
    "    \"avg_wer\": df_results[\"wer\"].mean(),\n",
    "    \"avg_cer\": df_results[\"cer\"].mean(),\n",
    "    \"avg_time_per_audio\": df_results[\"run_time_sec\"].mean(),\n",
    "    \"total_time\": df_results[\"run_time_sec\"].sum(),\n",
    "}\n",
    "\n",
    "print(\"large-v3_cuda_q\")\n",
    "print(json.dumps(summary, ensure_ascii=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8dfc5dfe-356e-4160-a6a6-899466b7c1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tiny</th>\n",
       "      <th>small</th>\n",
       "      <th>iarge-v3</th>\n",
       "      <th>large-v3_cuda</th>\n",
       "      <th>large-v3_cpu_quanted</th>\n",
       "      <th>large-v3_cuda_quanted</th>\n",
       "      <th>large-v3_cuda_autocast_compile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_samples</th>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "      <td>793.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_wer</th>\n",
       "      <td>1.049771</td>\n",
       "      <td>0.523011</td>\n",
       "      <td>0.440303</td>\n",
       "      <td>0.440303</td>\n",
       "      <td>0.473953</td>\n",
       "      <td>0.447231</td>\n",
       "      <td>0.442587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cer</th>\n",
       "      <td>0.486142</td>\n",
       "      <td>0.207796</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.166380</td>\n",
       "      <td>0.158552</td>\n",
       "      <td>0.158944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_time_per_audio</th>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.682818</td>\n",
       "      <td>7.948655</td>\n",
       "      <td>0.838627</td>\n",
       "      <td>5.805705</td>\n",
       "      <td>3.006851</td>\n",
       "      <td>0.896722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_time</th>\n",
       "      <td>133.937801</td>\n",
       "      <td>541.474906</td>\n",
       "      <td>6303.283459</td>\n",
       "      <td>665.031494</td>\n",
       "      <td>4603.923963</td>\n",
       "      <td>2384.432794</td>\n",
       "      <td>711.100724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          tiny       small     iarge-v3  large-v3_cuda  \\\n",
       "total_samples       793.000000  793.000000   793.000000     793.000000   \n",
       "avg_wer               1.049771    0.523011     0.440303       0.440303   \n",
       "avg_cer               0.486142    0.207796     0.158293       0.158293   \n",
       "avg_time_per_audio    0.168900    0.682818     7.948655       0.838627   \n",
       "total_time          133.937801  541.474906  6303.283459     665.031494   \n",
       "\n",
       "                    large-v3_cpu_quanted  large-v3_cuda_quanted  \\\n",
       "total_samples                 793.000000             793.000000   \n",
       "avg_wer                         0.473953               0.447231   \n",
       "avg_cer                         0.166380               0.158552   \n",
       "avg_time_per_audio              5.805705               3.006851   \n",
       "total_time                   4603.923963            2384.432794   \n",
       "\n",
       "                    large-v3_cuda_autocast_compile  \n",
       "total_samples                           793.000000  \n",
       "avg_wer                                   0.442587  \n",
       "avg_cer                                   0.158944  \n",
       "avg_time_per_audio                        0.896722  \n",
       "total_time                              711.100724  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"large-v3_cuda_autocast_compile\"] = summary\n",
    "\n",
    "with open(\"whisper_metric.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=True, indent=2)\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0662413d-7af0-4320-a1f7-b36ed0fc990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_a, input_features, predicted_ids, hypothesis, audio, audio_array\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13963db5-e1ae-41d5-a847-5de012156b62",
   "metadata": {},
   "source": [
    "### CUDA quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2fc9f8a-4143-4c5b-a794-297d855fc96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "qmodel_cuda_a = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\", \n",
    "                                                              load_in_8bit=True,   # или load_in_4bit=True\n",
    "                                                              device_map=\"auto\")\n",
    "qmodel_cuda_a.eval()\n",
    "qmodel_cuda_a = torch.compile(qmodel_cuda_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f102834-d010-419e-bde5-e3fd33b0d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/realn/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/realn/projects/EDLM_Whisper_Boost/venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfetto trace saved to whisper_perfetto_large-v3_CUDA_quantized_autocasted.json\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       whisper.generate         0.00%       0.000us         0.00%       0.000us       0.000us        5.273s      1115.12%        5.273s        5.273s           0 B           0 B           0 B           0 B             1  \n",
      "                                       whisper.generate         9.09%     479.332ms       100.00%        5.275s        5.275s       0.000us         0.00%     473.280ms     473.280ms           0 B        -855 B       1.28 MB      -5.46 GB             1  \n",
      "                                           MatMul8bitLt         7.38%     389.246ms        74.04%        3.906s     953.553us       0.000us         0.00%     259.307ms      63.307us           0 B           0 B       1.84 GB      -2.03 GB          4096  \n",
      "                     bitsandbytes::int8_mixed_scaled_mm         3.10%     163.407ms        31.14%        1.643s     401.129us       0.000us         0.00%     186.740ms      45.591us           0 B           0 B       1.84 GB      -1.15 GB          4096  \n",
      "                           bitsandbytes::int8_scaled_mm         2.91%     153.521ms        24.83%        1.310s     319.760us       0.000us         0.00%     180.231ms      44.002us           0 B           0 B       2.39 GB      -2.56 GB          4096  \n",
      "                       bitsandbytes::int8_linear_matmul         5.21%     274.866ms         7.70%     405.994ms      99.120us     126.286ms        26.71%     126.286ms      30.832us           0 B           0 B       2.56 GB           0 B          4096  \n",
      "void gemmSN_kernel_int32<256, 32, 4, 8, 8, 1, true, ...         0.00%       0.000us         0.00%       0.000us       0.000us     111.328ms        23.54%     111.328ms      31.063us           0 B           0 B           0 B           0 B          3584  \n",
      "                                           aten::conv1d         0.00%      68.656us         0.09%       4.723ms       1.181ms       0.000us         0.00%     109.694ms      27.424ms           0 B           0 B      21.98 MB     -11.05 MB             4  \n",
      "                     aten::scaled_dot_product_attention         0.48%      25.515ms         6.43%     339.367ms     174.482us       0.000us         0.00%     103.697ms      53.315us           0 B           0 B     223.19 MB    -188.60 MB          1945  \n",
      "                                            aten::copy_         3.01%     158.867ms         9.70%     511.493ms      33.019us      90.247ms        19.09%      90.260ms       5.827us           0 B           0 B           0 B           0 B         15491  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 5.275s\n",
      "Self CUDA time total: 472.853ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = profile_sample_autocast_cuda(116, trace_path=\"whisper_perfetto_large-v3_CUDA_quantized_autocasted.json\", model=qmodel_cuda_a, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c04bb4f-02a6-47bb-8b5f-5e44d5be122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del qmodel_cuda_a, _\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a6447-e2bf-4a7f-b0f2-33ed7e4736fc",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "На CPU- autocast накладывает огромное количество оверхэда на операции чистой и квантованной моделей и не должен использоваться в качестве ускорения инференса в данной работе.\n",
    "\n",
    "На чистом GPU оверхэд от autocast превышает выигрыш в скорости (что странно, но ожидаемо, т.к. в случае батчевания ситуация перебалансируется в сторону скорости).\n",
    "\n",
    "На квантованном GPU autocast не имеет смысла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cef747-1a81-4e59-bbaf-bbf6cc8f3bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
